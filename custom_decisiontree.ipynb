{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4581a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# File: custom_decisiontree.ipynb\\n# Author: Ryan Tso\\n# Created: Thur Jan 8, 2026\\n# Last Updated: Jan 8, 2026\\n\\nDescription: Building custom decision tree python script so I can prioritize the consideration of\\nmorphological features first then IHC features for the classification of RCC subtypes\\n    Approach - 2 stage gated model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# File: custom_decisiontree.ipynb\n",
    "# Author: Ryan Tso\n",
    "# Created: Thur Jan 8, 2026\n",
    "# Last Updated: Jan 12, 2026\n",
    "\n",
    "Description: Building custom decision tree python script so I can prioritize the consideration of\n",
    "morphological features first then IHC features for the classification of RCC subtypes\n",
    "    Approach - 2 stage gated model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4668d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d453e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    '''\n",
    "    Decision Tree Classifier\n",
    "        Training : use \"train\" function with train set features and labels\n",
    "        Predicting : Use \"predict\" function with test set features \n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1,\n",
    "                min_information_gain=0.0, numb_of_features_splitting=None): \n",
    "        '''\n",
    "        Setting the class with hyperparameters \n",
    "        (This constructor responsible for setting up initial state of new instance)\n",
    "        '''\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def _class_probabilities(self, labels:list) -> list: \n",
    "        '''\n",
    "        Calculate class / predicting label probability \n",
    "        Helps with calculating entropy and information gain \n",
    "            Ex: labels = [0,0,1,1,1]\n",
    "            probability = [0.4,0.6] --> 40% class 0, 60% class 1\n",
    "        '''\n",
    "        total_count = len(labels)\n",
    "        labels = np.array(labels) # convert to np array (vectorized) improves speed\n",
    "\n",
    "        print(f\"Total number of Samples: {total_count}\")\n",
    "\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        print(f\"Unique Labels: {unique_labels}\")\n",
    "        print(f\"Counts for each Label: {counts}\")\n",
    "\n",
    "        class_probability = counts / total_count\n",
    "\n",
    "        print(f\"Probability of Each Class: {class_probability}\")\n",
    "        return class_probability\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        '''\n",
    "        Calcuate shannon entropy \n",
    "        '''\n",
    "        entropy = 0                         # accumulator variable \n",
    "\n",
    "        for probability in class_probabilities:\n",
    "            if probability > 0:                     # avoids log2(0) -> undefined\n",
    "                p = probability\n",
    "                contribution = -p * np.log2(p)      # Shannon Entropy\n",
    "                print(f\"Contribution: {contribution}\")\n",
    "                entropy += contribution\n",
    "                print(f\"Entropy: {entropy}\")\n",
    "\n",
    "                # Production Code for faster performance\n",
    "                # entropy = sum(-p * np.log2(p) for p in class_probability if p > 0)\n",
    "        \n",
    "        print(f\"Final Entropy: {entropy}\")\n",
    "  \n",
    "        return entropy\n",
    "    \n",
    "    def _data_entropy(self, labels:list) -> float:\n",
    "        '''\n",
    "        Entropy calculated from raw labels using _class_prbabilities function\n",
    "        Combines the preprocessing step to calculate probabilities and entropy calculation\n",
    "        '''\n",
    "        data_entropy = self._entropy(self._class_probabilities(labels))\n",
    "                                \n",
    "        return data_entropy\n",
    "    \n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        return None\n",
    "\n",
    "    def _partition_entropy(self, subsets:list) -> float:\n",
    "        return None\n",
    "\n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        return None\n",
    "    \n",
    "    def _select_features_to_use(self, data:np.array) -> list:\n",
    "        return None\n",
    "    \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        return None\n",
    "    \n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        return None\n",
    "    \n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        return None\n",
    "\n",
    "    def _predict_one_sample(self, X:np.array) -> np.array:\n",
    "        return None\n",
    "    \n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        return None\n",
    "    \n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        return None\n",
    "    \n",
    "    # Tree Node requires an external import - see if we can replace... \n",
    "    def _print_recursive(self,node:TreeNode, level=0) -> None:\n",
    "        return None\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        return None\n",
    "\n",
    "    def _calculate_feature_importance(self,node):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c23789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Samples: 5\n",
      "Unique Labels: [0 1]\n",
      "Counts for each Label: [2 3]\n",
      "Probability of Each Class: [0.4 0.6]\n",
      "Contribution: 0.5287712379549449\n",
      "Entropy: 0.5287712379549449\n",
      "Contribution: 0.44217935649972373\n",
      "Entropy: 0.9709505944546686\n",
      "Final Entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "# Workspace before adding it into the main class - entropy\n",
    "import numpy as np\n",
    "\n",
    "labels = [0,0,1,1,1]\n",
    "\n",
    "total_count = len(labels)\n",
    "labels = np.array(labels) # convert to np array (vectorized) improves speed\n",
    "\n",
    "print(f\"Total number of Samples: {total_count}\")\n",
    "\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(f\"Unique Labels: {unique_labels}\")\n",
    "print(f\"Counts for each Label: {counts}\")\n",
    "\n",
    "class_probability = counts / total_count\n",
    "\n",
    "print(f\"Probability of Each Class: {class_probability}\")\n",
    "\n",
    "# Entropy Function\n",
    "entropy = 0                         # accumulator variable \n",
    "\n",
    "for probability in class_probability:\n",
    "    if probability > 0:                     # avoids log2(0) -> undefined\n",
    "        p = probability\n",
    "        contribution = -p * np.log2(p)      # Shannon Entropy\n",
    "        print(f\"Contribution: {contribution}\")\n",
    "        entropy += contribution\n",
    "        print(f\"Entropy: {entropy}\")\n",
    "\n",
    "        # Production Code for faster performance\n",
    "        # entropy = sum(-p * np.log2(p) for p in class_probability if p > 0)\n",
    "\n",
    "print(f\"Final Entropy: {entropy}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be89f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
